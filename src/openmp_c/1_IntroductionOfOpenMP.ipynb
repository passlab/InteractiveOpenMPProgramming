{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fd7c2f9-f29e-4057-982e-01836c6c6940",
   "metadata": {},
   "source": [
    "# Introduction of OpenMP\n",
    "In this section, we will first introduce what OpenMP is and some advantages of using OpenMP, and then we will introduce the execution model and memory model of OpenMP.\n",
    "\n",
    "## What is OpenMP?\n",
    "OpenMP is a standard parallel programming API for shared memory environments, written in C, C++, or FORTRAN.\n",
    "It consists of a set of compiler directives with a \"lightweight\" syntax, library routines, and environment variables that influence run-time behavior. OpenMP is governed by OpenMP Architecture Review Board (or OpenMP ARB), and defined by a number of hardware and software vendors.\n",
    "\n",
    "OpenMP behavoir is directly dependent on the OpenMP implementation. Capabilties of this implementation can enable the programmer to seperate program into serial and parallel reigons rather than just concurrently running threads, hides stack managemend, and provides synchronization of constructs. That being said OpenMP will not garuntee speedup, parallelize dependencies, or prevent data racing. Data racing, keeping track of dependencies, and working towards a speedup are all up to the programmer.\n",
    "\n",
    "## Why we use OpenMP?\n",
    "OpenMP has received considerable attention in the past decade and is considered by many to be an ideal solution for parallel programming, because it has unique advantages as a mainstream diretcive-based programming model.\n",
    "\n",
    "First of all, OpenMP provides a cross-platform, cross-compiler solution. It supports lots of platforms such as Linux, macOS, and Windows. Mainstream compilers including GCC, LLVM/Clang, Intel Fortran and C/C++ compilers provide OpenMP good support. Also, with the rapid development of OpenMP, many researchers and computer vendors are constantly exploring how to optimize the execution efficiency of OpenMP programs, and continue to propose improvements for exiting compilers or develop new compilers. What's more. OpenMP is a standard specification, and all compilers that support it implement the same set of standards, and there are no portability issues.\n",
    "\n",
    "Secondly, using Openmp can be very convenient and flexible to modify the number of threads. To solve the scalability problem of the number of CPU cores. In the multi-core era, the number of threads needs to change according to the number of CPU cores. OpenMP has irreplaceable advantages in this regard.\n",
    "\n",
    "Thirdly, using OpenMP to create threads is considered to be convenient and relatively easy because it does not require an entry function, the code within the same function can be decomposed into multiple threads for execution, and a for loop can be decomposed into multiple threads for execution. If OpenMP is not used, when the operating system API creates a thread, the code in a function needs to be manually disassembled into multiple thread entry functions.\n",
    "\n",
    "To sum up, OpenMP has irreplaceable advantages in parallel programming. More and more new diretcives are being added to achieve more functions, and they are playing an important role on many different platforms.\n",
    "\n",
    "## OpenMP Excution Model \n",
    "\n",
    "The parallel model used by OpenMP is called the fork-join model, as shown in the following figure:\n",
    "![fork-join_model1](20190720101130709.png \"topic1\")\n",
    "![fork-join_model1](0_13209261094r4F.png \"topic2\")\n",
    "\n",
    "<font color=\"#000066\"> Drwa a new figure, merge these 2 figures and shown the barrier in the new figure</font><br />\n",
    "When the master thread which is executing a serial region encounters a parallel region, i.e. when it encounters an OpenMP parallel instruction, it creates a thread group consisting of itself and some additional (possibly zero) worker threads.\n",
    "These threads can execute tasks in parallel and are uniformly dispatched by the master thread. There is an implicit barrier at the end of the parallel region. When the thread has finished executing its task, it will wait at the barrier. When all threads have completed their tasks, the threads can leave the barrier. The master thread continues to execute the serial code after the parallel region.\n",
    "\n",
    "Thread management is done by the runtime library, which maintains a pool of worker threads that can be used to work on parallel regions. It will allocate or reclaim the threads based on the OpenMP instructions and sometimes adjust the number of threads to allocate based on the availablity of threads.\n",
    "\n",
    "## OpenMP Memory Model\n",
    "\n",
    "At the beginning of this section, we briefly introduce two common memory models, namely shared memory and distributed memory. Shared memory means that multiple cores share a single memory, while distributed memory means that each compute node (possibly one or more cores) has its own memory. General large computers combine distributed memory and shared memory models, that is, shared memory within each computing node and distributed memory between nodes.\n",
    "\n",
    "OpenMP supports shared memory model that reduces execution time by creating multithreading to distribute parallelizable loads to multiple physical computing cores. \n",
    "Before the support for heterogeneous computing in OpenMP 4.0, there was only one address space.\n",
    "\n",
    "OpenMP programs have two different basic types of memory: private and shared. Variables can also be divided into private variables and public variables. The type of the variable can be defined using an explicit private clause, or it can be defined according to default rules. Although the default rules are clearly, we still recommend programmers to explicitly define the types of variables, because using default rules does not always meet our expectations, and many errors may occur because of the subtlety of default rules.\n",
    "The following figure shows the OpenMP memory model. \n",
    "\n",
    "![memory_model](1.png \"topic3\")\n",
    "\n",
    "Private variables are stored in private memory that is specially allocated to each thread, and other threads cannot access or modify private variables. Access conflicts will never happen when accessing the private variables. Even if there are other variables with the same name in the program, or variables with the same name for multiple threads, these variables will be stored in different locations and will not be confused. The specific mechanism will be explained in detail when introducing the private clause.\n",
    "\n",
    "There are two differences between shared variables and private variables. The first is that shared variables are not allowed to share one name. Each variable is unique. Secondly, shared variables are stored in memory that all threads can access, and any thread can access shared variables at any time. The control of access conflicts is the responsibility of the user."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Native",
   "language": "native",
   "name": "native"
  },
  "language_info": {
   "file_extension": ".c",
   "mimetype": "text/plain",
   "name": "c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
