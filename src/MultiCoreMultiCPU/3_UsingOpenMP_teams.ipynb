{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d6f12df",
   "metadata": {},
   "source": [
    "# Creating SPMD parallelism using OpenMP `teams` directive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22c5131",
   "metadata": {},
   "source": [
    "In OpenMP, the `teams` directive plays a crucial role, especially in the context of offloading computations to devices such as GPUs.\n",
    "\n",
    "The `teams` directive in OpenMP is used to create a league of thread teams, each of which can execute concurrently. This directive is particularly useful in scenarios where a hierarchical parallelism model is desired. For example, in GPU programming, a common pattern is to distribute work among multiple teams of threads, where each team can further parallelize the work among its member threads.\n",
    "\n",
    "Unlike the `parallel` directive, which is used to create a single team of threads, the `teams` directive allows for the creation of multiple teams, providing an additional level of parallelism. This makes the `teams` directive a powerful tool for exploiting the parallel capabilities of modern hardware architectures, especially in the context of heterogeneous computing.\n",
    "\n",
    "In this chapter, we will explore the basic usage of the `teams` directive, its interaction with other OpenMP directives, and how it can be used to target parallel execution on devices like GPUs. By the end of this chapter, you should have a solid understanding of the `teams` directive and how to leverage it to write efficient parallel programs with OpenMP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18848ac3",
   "metadata": {},
   "source": [
    "## Basic Usage\n",
    "\n",
    "### `teams` Directive\n",
    "The `teams` directive indicates that the loop that follows is split among multiple thread teams, one thread team computing one part of the task. Developers can use the `teams` directive to use a large number of thread teams.\n",
    "\n",
    "The following figure shows the execution model of the `teams` directive:\n",
    "\n",
    "![teams_directive](teams.jpeg \"topic1\")\n",
    "\n",
    "A league of teams is created when a thread encounters a `teams` construct. Each team is an initial team, and the initial thread in each team executes the team area.\n",
    "After a team is created, the number of initial teams remains the same for the duration of the `teams` region.\n",
    "Within a `teams` region, the initial team number uniquely identifies each initial team. A thread can obtain its own initial team number by calling the *omp_get_team_num* library routine.\n",
    "The teams directive has the following characteristics:\n",
    "- the `teams` directive can spawn one or more thread teams with the same number of threads\n",
    "- code is portable for one thread team or multiple thread teams\n",
    "- only the primary thread of each team continues to execute\n",
    "- no synchronization between thread teams\n",
    "- programmers don't need to think about how to decompose loops\n",
    "\n",
    "OpenMP was originally designed for multithreading on shared-memory parallel computers, so the parallel directive only creates a single layer of parallelism.\n",
    "The team instruction is used to express the second level of scalable parallelization. Before OpenMP 5.0, it can be only used on the GPU (with an associated target construct). In OpenMP 5.0 the `teams` construct was extended to enable the host to execute a teams region.\n",
    "\n",
    "The syntax for the `teams` directive in C/C++ is:\n",
    "\n",
    "```c\n",
    "#pragma omp teams [clause[ [,] clause] ... ] new-line\n",
    "    structured-block\n",
    "```\n",
    "\n",
    "The syntax in Fortran is:\n",
    "\n",
    "```fortran\n",
    "!$omp teams [clause[ [,] clause] ... ]\n",
    "    loosely-structured-block\n",
    "!$omp end teams\n",
    "```\n",
    "\n",
    "In its simplest form, the `teams` directive can be used without any clauses:\n",
    "\n",
    "```c\n",
    "#pragma omp teams\n",
    "{\n",
    "    // Code to be executed by each team\n",
    "}\n",
    "```\n",
    "\n",
    "This will create a league of teams, with the number of teams and the number of threads per team determined by the implementation. Typically, the number of teams is chosen to match the number of available processing units, such as cores or GPU compute units.\n",
    "\n",
    "### Example: Creating Teams\n",
    "\n",
    "Here's a basic example that demonstrates the use of the `teams` directive:\n",
    "\n",
    "```c\n",
    "#include <stdio.h>\n",
    "#include <omp.h>\n",
    "\n",
    "int main() {\n",
    "    #pragma omp teams\n",
    "    {\n",
    "        printf(\"Team %d out of %d teams\\n\", omp_get_team_num(), omp_get_num_teams());\n",
    "    }\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "In this example, each team will print its team number and the total number of teams. The `omp_get_team_num()` function returns the team number of the calling thread, and the `omp_get_num_teams()` function returns the total number of teams in the current league.\n",
    "\n",
    "When executed, the program might produce output similar to the following, depending on the number of teams created by the implementation:\n",
    "\n",
    "```\n",
    "Team 0 out of 4 teams\n",
    "Team 1 out of 4 teams\n",
    "Team 2 out of 4 teams\n",
    "Team 3 out of 4 teams\n",
    "```\n",
    "\n",
    "This simple example demonstrates the basic usage of the `teams` directive to create multiple teams of threads. In the following sections, we will explore how to control the number of teams and threads, and how the `teams` directive interacts with other OpenMP directives for more complex parallel programming scenarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463500c6-2694-4874-af2f-70fcc0ac9987",
   "metadata": {},
   "source": [
    "## Interaction with Other Directives\n",
    "\n",
    "The `teams` directive in OpenMP is often used in conjunction with other directives to create a hierarchical parallelism model. This section explores how the `teams` directive interacts with the `distribute`, `parallel`, and `target` directives.\n",
    "\n",
    "### Teams and Distribute Directives\n",
    "\n",
    "The `distribute` directive is used to distribute the iterations of a loop across the teams created by the `teams` directive. This combination is particularly useful for data parallelism where each team works on a different portion of the data.\n",
    "\n",
    "**Example:** Distributing loop iterations across teams to perform a parallel reduction.\n",
    "\n",
    "```c\n",
    "#include <omp.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "int main() {\n",
    "    const int N = 1000;\n",
    "    int array[N];\n",
    "    int sum = 0;\n",
    "\n",
    "    // Initialize the array with some values\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        array[i] = i;\n",
    "    }\n",
    "\n",
    "    #pragma omp target teams distribute reduction(+:sum)\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        sum += array[i];\n",
    "    }\n",
    "\n",
    "    printf(\"Sum: %d\\n\", sum);\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "In this example, the loop iterations are distributed across teams, and each team contributes to the reduction operation to calculate the sum of the array elements.\n",
    "\n",
    "### Teams and Parallel Directives\n",
    "\n",
    "The `teams` directive can also be combined with the `parallel` directive to create nested parallelism. Within each team, the `parallel` directive creates a parallel region where multiple threads can work concurrently.\n",
    "\n",
    "**Example:** Using nested parallelism with the `teams` and `parallel` directives to perform matrix multiplication.\n",
    "\n",
    "```c\n",
    "#include <omp.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "#define N 1000\n",
    "#define M 1000\n",
    "#define P 1000\n",
    "\n",
    "int main() {\n",
    "    double A[N][M], B[M][P], C[N][P];\n",
    "\n",
    "    // Initialize matrices A and B\n",
    "    // ...\n",
    "\n",
    "    #pragma omp target teams\n",
    "    {\n",
    "        #pragma omp distribute parallel for collapse(2)\n",
    "        for (int i = 0; i < N; i++) {\n",
    "            for (int j = 0; j < P; j++) {\n",
    "                double sum = 0.0;\n",
    "                for (int k = 0; k < M; k++) {\n",
    "                    sum += A[i][k] * B[k][j];\n",
    "                }\n",
    "                C[i][j] = sum;\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Print matrix C\n",
    "    // ...\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "In this example, the outer loop iterations (over `i` and `j`) are distributed across teams, and within each team, the loop iterations are executed in parallel by the team's threads.\n",
    "\n",
    "### Teams and Target Directives\n",
    "\n",
    "The `teams` directive is commonly used with the `target` directive for offloading computations to a device, such as a GPU. The `target` directive specifies that the code block should be executed on the device, and the `teams` directive creates teams of threads on the device to execute the code.\n",
    "\n",
    "**Example:** Offloading a computation to a device and using teams to perform the computation in parallel.\n",
    "\n",
    "```c\n",
    "#include <omp.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "#define N 1000\n",
    "\n",
    "int main() {\n",
    "    float A[N], B[N], C[N];\n",
    "\n",
    "    // Initialize arrays A and B\n",
    "    // ...\n",
    "\n",
    "    #pragma omp target teams map(to: A, B) map(from: C)\n",
    "    {\n",
    "        #pragma omp distribute parallel for\n",
    "        for (int i = 0; i < N; i++) {\n",
    "            C[i] = A[i] + B[i];\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Print array C\n",
    "    // ...\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "In this example, the computation of the element-wise sum of two arrays is offloaded to a device. The `teams` directive is used to create teams of threads on the device, and the `distribute parallel for` construct is used to distribute the loop iterations across the teams and execute them in parallel.\n",
    "\n",
    "### Summary\n",
    "\n",
    "The `teams` directive provides a flexible way to create a hierarchical parallelism model in OpenMP. By combining the `teams` directive with the `distribute`, `parallel`, and `target` directives, developers can efficiently utilize the parallel capabilities of both CPUs and GPUs for a wide range of applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8d1f71-032d-484f-a98c-b9fc8745b98a",
   "metadata": {},
   "source": [
    "## Data Environment\n",
    "\n",
    "The `teams` directive in OpenMP creates a new data environment for the region of code it encloses. This section discusses the data-sharing attributes and other aspects of the data environment created by the `teams` directive.\n",
    "\n",
    "### Data-Sharing Attributes\n",
    "\n",
    "Within a `teams` region, variables can have one of the following data-sharing attributes: shared, private, firstprivate, or reduction. The default data-sharing attribute for variables in a `teams` region is determined by the default clause if present; otherwise, it follows the rules of the OpenMP specification.\n",
    "\n",
    "- **Shared:** Variables with the shared attribute are shared among all threads in all teams. Each team accesses the same storage location for shared variables.\n",
    "- **Private:** Variables with the private attribute have a separate copy for each thread. Each thread in each team has its own copy of private variables.\n",
    "- **Firstprivate:** Similar to private, but each thread's copy is initialized with the value of the variable before entering the `teams` region.\n",
    "- **Reduction:** Variables with the reduction attribute are subject to a reduction operation at the end of the `teams` region.\n",
    "\n",
    "### Example: Data-Sharing Attributes in Teams Region\n",
    "\n",
    "```c\n",
    "#include <omp.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "int main() {\n",
    "    int shared_var = 100;\n",
    "    int private_var = 200;\n",
    "\n",
    "    #pragma omp target teams map(shared_var) private(private_var)\n",
    "    {\n",
    "        // Inside the teams region\n",
    "        private_var += omp_get_team_num();\n",
    "        #pragma omp parallel reduction(+:shared_var)\n",
    "        {\n",
    "            shared_var += private_var;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    printf(\"Shared variable: %d\\n\", shared_var);\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "In this example, `shared_var` is shared among all teams and threads, while `private_var` is private to each thread. The value of `shared_var` is modified through a reduction operation within the parallel region inside the teams region.\n",
    "\n",
    "### Summary\n",
    "\n",
    "Understanding the data environment and data-sharing attributes in a `teams` region is crucial for writing correct and efficient parallel programs with OpenMP. Properly managing the data environment ensures that data is correctly shared or privatized among teams and threads, avoiding data races and other concurrency issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f51b6ff-cb99-4636-bbc9-e47852385675",
   "metadata": {},
   "source": [
    "## Clauses\n",
    "\n",
    "The `teams` directive in OpenMP can be used with several clauses to control the behavior of the teams and the data environment. This section discusses the clauses with the `teams` directive.\n",
    "\n",
    "### num_teams Clause\n",
    "\n",
    "**Syntax:** `num_teams(integer-expression)`\n",
    "\n",
    "The `num_teams` clause specifies the number of teams to be created in the teams region.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```c\n",
    "#include <omp.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "int main() {\n",
    "    #pragma omp target teams num_teams(4)\n",
    "    {\n",
    "        printf(\"Team %d out of %d teams\\n\", omp_get_team_num(), omp_get_num_teams());\n",
    "    }\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "### thread_limit Clause\n",
    "\n",
    "**Syntax:** `thread_limit(integer-expression)`\n",
    "\n",
    "The `thread_limit` clause specifies the maximum number of threads that can be part of each team.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```c\n",
    "#include <omp.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "int main() {\n",
    "    #pragma omp target teams thread_limit(8)\n",
    "    {\n",
    "        #pragma omp parallel\n",
    "        {\n",
    "            printf(\"Thread %d in team %d\\n\", omp_get_thread_num(), omp_get_team_num());\n",
    "        }\n",
    "    }\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "### reduction Clause\n",
    "\n",
    "**Syntax:** `reduction(operator: list)`\n",
    "\n",
    "The `reduction` clause is used to perform a reduction operation on variables that are private to each thread but shared across the teams.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```c\n",
    "#include <omp.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "int main() {\n",
    "    int sum = 0;\n",
    "\n",
    "    #pragma omp target teams map(tofrom: sum) reduction(+:sum)\n",
    "    {\n",
    "        #pragma omp parallel\n",
    "        {\n",
    "            sum += omp_get_thread_num();\n",
    "        }\n",
    "    }\n",
    "\n",
    "    printf(\"Sum: %d\\n\", sum);\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "### default Clause\n",
    "\n",
    "**Syntax:** `default(shared | none)`\n",
    "\n",
    "The `default` clause sets the default data-sharing attribute for variables in the teams region.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```c\n",
    "#include <omp.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "int main() {\n",
    "    #pragma omp target teams default(none)\n",
    "    {\n",
    "        int x;\n",
    "        // Code that uses the variable x\n",
    "    }\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "### allocate Clause\n",
    "\n",
    "**Syntax:** `allocate([allocator:] list)`\n",
    "\n",
    "The `allocate` clause specifies the allocator to be used for the variables in the list.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```c\n",
    "#include <omp.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "int main() {\n",
    "    #pragma omp target teams allocate(x)\n",
    "    {\n",
    "        // Code that uses the variable x\n",
    "    }\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "### shared Clause\n",
    "\n",
    "**Syntax:** `shared(list)`\n",
    "\n",
    "The `shared` clause specifies that the data within a parallel region is shared among all threads.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```c\n",
    "#include <omp.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "int main() {\n",
    "    #pragma omp target teams shared(x)\n",
    "    {\n",
    "        // All threads in the team share the variable x\n",
    "    }\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "### private Clause\n",
    "\n",
    "**Syntax:** `private(list)`\n",
    "\n",
    "The `private` clause specifies that each thread should have its own instance of a variable.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```c\n",
    "#include <omp.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "int main() {\n",
    "    #pragma omp target teams private(x)\n",
    "    {\n",
    "        // Each thread has its own private copy of x\n",
    "    }\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "### firstprivate Clause\n",
    "\n",
    "**Syntax:** `firstprivate(list)`\n",
    "\n",
    "The `firstprivate` clause provides each thread with its own copy of a variable and initializes each copy with the value of the variable at the time the parallel region is entered.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```c\n",
    "#include <omp.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "int main() {\n",
    "    #pragma omp target teams firstprivate(x)\n",
    "    {\n",
    "        // Each thread has its own copy of x, initialized with the value of x\n",
    "    }\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "### Summary\n",
    "\n",
    "Clauses provide a powerful way to customize the behavior of the `teams` directive in OpenMP. By using clauses like `num_teams`, `thread_limit`, `reduction`, `default`, `allocate`, `shared`, `private`, and `firstprivate`, programmers can control various aspects of the teams' behavior and the data environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d39d0c9-4948-4868-87a0-f94bd39138a9",
   "metadata": {},
   "source": [
    "## Targeting Devices\n",
    "\n",
    "In modern computing, harnessing the power of specialized hardware such as Graphics Processing Units (GPUs) is essential for achieving high performance in parallel computing. OpenMP provides a seamless way to offload computations to these devices using the `target` and `teams` directives.\n",
    "\n",
    "### Offloading with Target and Teams Directives\n",
    "\n",
    "The `target` directive instructs the compiler to execute the enclosed code block on a specified device, typically a GPU. The `teams` directive, when used in conjunction with `target`, creates a league of thread teams to execute the code in parallel on the device.\n",
    "\n",
    "**Example: Vector Addition on a GPU**\n",
    "\n",
    "Consider the following example where two arrays are added element-wise and the result is stored in a third array:\n",
    "\n",
    "```c\n",
    "#include <omp.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "int main() {\n",
    "    const int N = 1000;\n",
    "    float a[N], b[N], c[N];\n",
    "\n",
    "    // Initialize the input arrays\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        a[i] = i * 1.0f;\n",
    "        b[i] = i * 2.0f;\n",
    "    }\n",
    "\n",
    "    // Offload the computation to a GPU\n",
    "    #pragma omp target teams map(to: a, b) map(from: c)\n",
    "    {\n",
    "        #pragma omp distribute parallel for\n",
    "        for (int i = 0; i < N; i++) {\n",
    "            c[i] = a[i] + b[i];\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Output the result\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        printf(\"%f \", c[i]);\n",
    "    }\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "In this example, the `target teams` directive offloads the addition of arrays `a` and `b` to a GPU. The `map` clauses specify how data is transferred between the host and the device. The `distribute parallel for` directive ensures that the computation is performed in parallel by the teams of threads on the device.\n",
    "\n",
    "### Leveraging Device Parallelism\n",
    "\n",
    "Using the `teams` directive with the `target` directive enables you to harness the parallel processing capabilities of devices like GPUs. It allows for scalable parallel execution where the workload is distributed across multiple teams of threads, each executing concurrently on the device.\n",
    "\n",
    "### Summary\n",
    "\n",
    "The integration of the `teams` directive with the `target` directive in OpenMP provides a powerful and flexible approach to offloading computations to devices. This capability allows programmers to efficiently utilize the parallel processing power of GPUs and other devices, thereby enhancing the performance of parallel applications. In the following chapters, we will delve deeper into the `map` clause and explore other aspects of device offloading in OpenMP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16395d7a-5314-4c55-9c6d-0405abf0c703",
   "metadata": {},
   "source": [
    "## Best Practices for Effective Use of Teams Directive\n",
    "\n",
    "Optimizing the use of the `teams` directive in OpenMP is crucial for harnessing the full potential of parallel computing, especially when offloading computations to devices such as GPUs. Here are some essential best practices to keep in mind:\n",
    "\n",
    "### Optimal Team Configuration\n",
    "\n",
    "Selecting the appropriate number of teams is pivotal. It should be aligned with the target device's architecture and the computational workload. For GPUs, a rule of thumb is to create a number of teams that corresponds to a multiple of the device's compute units.\n",
    "\n",
    "### Workload Distribution\n",
    "\n",
    "Achieving a balanced distribution of work among teams is key to avoiding performance bottlenecks. Utilize the `distribute` directive in tandem with `teams` to ensure an equitable assignment of loop iterations or other computational tasks across the teams.\n",
    "\n",
    "**Example: Workload Distribution**\n",
    "\n",
    "```c\n",
    "#include <omp.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "#define N 10000\n",
    "\n",
    "int main() {\n",
    "    float array[N];\n",
    "\n",
    "    // Initialize the array\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        array[i] = i;\n",
    "    }\n",
    "\n",
    "    // Distribute workload evenly among teams\n",
    "    #pragma omp target teams map(tofrom: array)\n",
    "    #pragma omp distribute\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        array[i] = array[i] * 2.0f;\n",
    "    }\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "### Minimizing Data Movement\n",
    "\n",
    "Data transfer between the host and the device can significantly impact performance. Employ the `map` clause with precision to reduce unnecessary data movement. Additionally, explore techniques like data prefetching and overlapping data transfers with computation to enhance data locality.\n",
    "\n",
    "**Example: Efficient Data Movement**\n",
    "\n",
    "```c\n",
    "#include <omp.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "#define N 10000\n",
    "\n",
    "int main() {\n",
    "    float input[N], output[N];\n",
    "\n",
    "    // Initialize the input array\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        input[i] = i;\n",
    "    }\n",
    "\n",
    "    // Minimize data movement in offloading\n",
    "    #pragma omp target teams map(to: input) map(from: output)\n",
    "    {\n",
    "        #pragma omp distribute parallel for\n",
    "        for (int i = 0; i < N; i++) {\n",
    "            output[i] = input[i] * 2.0f;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "### Efficient Reductions\n",
    "\n",
    "Ensure that reductions are performed efficiently, especially when using the `reduction` clause with the `teams` directive. Implement parallel reduction algorithms that leverage the device's parallel processing capabilities to avoid bottlenecks.\n",
    "\n",
    "**Example: Parallel Reduction**\n",
    "\n",
    "```c\n",
    "#include <omp.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "#define N 10000\n",
    "\n",
    "int main() {\n",
    "    float array[N];\n",
    "    float sum = 0.0f;\n",
    "\n",
    "    // Initialize the array\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        array[i] = i;\n",
    "    }\n",
    "\n",
    "    // Efficient reduction on the device\n",
    "    #pragma omp target teams map(to: array) reduction(+:sum)\n",
    "    {\n",
    "        #pragma omp distribute parallel for\n",
    "        for (int i = 0; i < N; i++) {\n",
    "            sum += array[i];\n",
    "        }\n",
    "    }\n",
    "\n",
    "    printf(\"Sum: %f\\n\", sum);\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "### Debugging and Profiling\n",
    "\n",
    "Leverage OpenMP-aware debugging and profiling tools to identify and address performance bottlenecks. Regularly verify the correctness of parallel execution to ensure the expected behavior of your application.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Adhering to these best practices will enable you to maximize the efficiency and correctness of your parallel applications using the `teams` directive in OpenMP. Tailor your approach to the specific requirements of your application and the capabilities of your target device for optimal results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb90664-069e-4702-b56a-fdf287072021",
   "metadata": {},
   "source": [
    "## Advanced Topics\n",
    "\n",
    "While the basic usage of the `teams` directive provides a powerful tool for parallel computing, there are several advanced topics that can further enhance the performance and flexibility of your OpenMP applications. Here are some advanced topics related to the `teams` directive:\n",
    "\n",
    "### Nested Parallelism\n",
    "\n",
    "OpenMP supports nested parallelism, where a parallel region can be defined inside another parallel region. This can be particularly useful when using the `teams` directive to create a hierarchical parallelism model.\n",
    "\n",
    "**Example: Nested Parallelism**\n",
    "\n",
    "```c\n",
    "#include <omp.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "int main() {\n",
    "    #pragma omp target teams\n",
    "    {\n",
    "        // Outer parallel region executed by teams\n",
    "        #pragma omp parallel\n",
    "        {\n",
    "            // Inner parallel region executed by threads within each team\n",
    "            printf(\"Team %d, Thread %d\\n\", omp_get_team_num(), omp_get_thread_num());\n",
    "        }\n",
    "    }\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "In this example, an outer parallel region is created by the `teams` directive, and an inner parallel region is created by the `parallel` directive. Each team of threads executes the inner parallel region independently.\n",
    "\n",
    "### SIMD Parallelism\n",
    "\n",
    "The `teams` directive can be combined with SIMD (Single Instruction, Multiple Data) constructs to exploit vector parallelism on devices that support SIMD instructions.\n",
    "\n",
    "**Example: SIMD Parallelism**\n",
    "\n",
    "```c\n",
    "#include <omp.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "#define N 10000\n",
    "\n",
    "int main() {\n",
    "    float array[N];\n",
    "\n",
    "    // Initialize the array\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        array[i] = i;\n",
    "    }\n",
    "\n",
    "    // Use SIMD parallelism within teams\n",
    "    #pragma omp target teams map(tofrom: array)\n",
    "    #pragma omp distribute\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        #pragma omp simd\n",
    "        for (int j = 0; j < N; j++) {\n",
    "            array[j] = array[j] + 1.0f;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "In this example, the `simd` directive is used within the loop to enable SIMD parallelism, allowing multiple elements of the array to be processed simultaneously by SIMD instructions.\n",
    "\n",
    "### Interoperability with Other Programming Models\n",
    "\n",
    "OpenMP can interoperate with other programming models, such as MPI (Message Passing Interface) or CUDA, to take advantage of specific features of different parallel programming approaches.\n",
    "\n",
    "**Example: Interoperability with MPI**\n",
    "\n",
    "```c\n",
    "#include <omp.h>\n",
    "#include <mpi.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "int main(int argc, char *argv[]) {\n",
    "    MPI_Init(&argc, &argv);\n",
    "\n",
    "    int rank;\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "\n",
    "    #pragma omp target teams\n",
    "    {\n",
    "        printf(\"MPI Rank %d, Team %d\\n\", rank, omp_get_team_num());\n",
    "    }\n",
    "\n",
    "    MPI_Finalize();\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "In this example, OpenMP is used in conjunction with MPI, where each MPI process executes an OpenMP `teams` region on a device. This allows for a hybrid parallel programming approach that combines distributed memory parallelism (MPI) with shared memory parallelism (OpenMP).\n",
    "\n",
    "### Summary\n",
    "\n",
    "Exploring advanced topics related to the `teams` directive can unlock new levels of performance and flexibility in your OpenMP applications. Whether it's leveraging nested parallelism, exploiting SIMD instructions, or interoperating with other programming models, these advanced techniques can help you tackle complex parallel computing challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cc6c54-4640-453e-8956-bfdc4d80ee18",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The `teams` directive is a powerful feature in OpenMP that enables the creation of a league of thread teams for parallel execution, particularly on devices such as GPUs. This chapter has covered the fundamental concepts, usage, and best practices associated with the `teams` directive, along with advanced topics for further exploration.\n",
    "\n",
    "Key takeaways from this chapter include:\n",
    "\n",
    "- **Basic Usage:** The `teams` directive, often used in conjunction with the `target` directive, allows for offloading computations to a device with a specified number of teams and threads.\n",
    "- **Data Environment:** Understanding and managing the data environment, including data-sharing attributes and clauses like `allocate`, `default`, `shared`, and `private`, is crucial for efficient parallel execution.\n",
    "- **Targeting Devices:** The combination of `target` and `teams` directives provides a flexible and powerful approach to offload computations to devices, with the `map` clause playing a vital role in managing data movement.\n",
    "- **Best Practices:** To maximize performance and ensure correct execution, it's important to choose the right number of teams, balance the workload among teams, minimize data movement, use reductions efficiently, and employ debugging and profiling tools.\n",
    "- **Advanced Topics:** Exploring nested parallelism, SIMD parallelism, and interoperability with other programming models can further enhance the capabilities of your OpenMP applications.\n",
    "\n",
    "By mastering the use of the `teams` directive, you can unlock new levels of parallelism and performance in your applications, making them more scalable and efficient on modern computing architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f6c8da-534b-4213-9d04-ce3825a2ec93",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Nested Parallelism:** Modify the example code for nested parallelism to create a hierarchy of parallel regions using the `teams` directive. Experiment with different numbers of teams and threads to observe the performance impact.\n",
    "\n",
    "2. **SIMD Parallelism:** Implement a matrix multiplication algorithm using SIMD parallelism within the `teams` directive. Compare the performance of the SIMD-parallelized version with a non-parallelized version.\n",
    "\n",
    "3. **Load Balancing:** Write a program that performs a computation on a large array and uses the `distribute` directive to distribute the workload among teams. Experiment with different load balancing strategies to minimize load imbalance.\n",
    "\n",
    "4. **Reduction Operation:** Modify the example code for the reduction clause to perform a more complex reduction operation, such as finding the maximum or minimum value in an array, using the `teams` directive.\n",
    "\n",
    "5. **Interoperability:** Explore interoperability between OpenMP and another parallel programming model, such as MPI or CUDA. Write a program that combines both programming models to perform a parallel computation.\n",
    "\n",
    "6. **Debugging and Profiling:** Use an OpenMP-aware debugging or profiling tool to analyze the performance of one of the previous exercises. Identify any performance bottlenecks and propose optimizations.\n",
    "\n",
    "7. **Optimization Strategies:** Experiment with different optimization strategies, such as loop unrolling, loop tiling, or data prefetching, within the `teams` directive to improve the performance of a parallelized computation.\n",
    "\n",
    "8. **Real-world Application:** Identify a real-world application that can benefit from parallelization using the `teams` directive. Write a program that parallelizes a relevant part of the application and measure the performance improvement.\n",
    "\n",
    "9. **Scalability Analysis:** Conduct a scalability analysis of one of the previous exercises by varying the problem size and the number of teams/threads. Determine the scalability limits of your parallelized code.\n",
    "\n",
    "10. **Advanced Topics Exploration:** Choose one of the advanced topics discussed in this chapter, such as nested parallelism or SIMD parallelism, and explore it further by implementing a more complex example or conducting a detailed performance analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458eafdb-0359-438b-ad9d-c8055cfa52e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
