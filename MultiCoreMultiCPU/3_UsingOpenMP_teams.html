
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>2.3. Creating SPMD parallelism using OpenMP teams directive &#8212; Interactive OpenMP Programming Book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=87e54e7c" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'MultiCoreMultiCPU/3_UsingOpenMP_teams';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="2.4. Synchronization of Threads Using Barrier and Ordered Directive" href="4_SynchronizationThreads.html" />
    <link rel="prev" title="2.2. Creating SPMD parallelism using OpenMP parallel directive" href="2_UsingOpenMP_parallel.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../cover.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Interactive OpenMP Programming Book - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Interactive OpenMP Programming Book - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../cover.html">
                    Interactive OpenMP Programming Book
                </a>
            </li>
        </ul>
        <ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../foreword.html">Preface</a></li>
</ul>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../Ch1_OpenmpIntro.html">1. Overview of OpenMP Programming</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Openmp_C/1_IntroductionOfOpenMP.html">1.1. Introduction of OpenMP</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Openmp_C/2_Syntax.html">1.2. Creating a Parallel Program with OpenMP</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Openmp_C/3_Performance.html">1.3. Performance Analysis</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../Ch2_MulticoreMultiCPU.html">2. Parallel Programming for Multicore and Multi-CPU Machines</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="1_MIMDArchitecture.html">2.1. Multicore and Multi-CPU shared memory systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="2_UsingOpenMP_parallel.html">2.2. Creating SPMD parallelism using OpenMP <strong>parallel</strong> directive</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">2.3. Creating SPMD parallelism using OpenMP <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive</a></li>
<li class="toctree-l2"><a class="reference internal" href="4_SynchronizationThreads.html">2.4. Synchronization of Threads Using Barrier and Ordered Directive</a></li>
<li class="toctree-l2"><a class="reference internal" href="4_SynchronizationThreads_Gemini.html">2.5. Synchronization of Threads Using Barrier and Ordered Directive</a></li>
<li class="toctree-l2"><a class="reference internal" href="4_SynchronizationThreads_Claude.html">2.6. Synchronization of Threads Using Barrier and Ordered Directive</a></li>
<li class="toctree-l2"><a class="reference internal" href="5_AsynchronousTasking.html">2.7. Asynchronous Tasking</a></li>
<li class="toctree-l2"><a class="reference internal" href="6_ExplicitDistribution.html">2.8. Explicit Distribution of Work Using Single, Sections, Workshring-Loop, and Distribute Construct</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Ch3_SIMDVector.html">3. Parallel Programming for SIMD and Vector Architecture</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../SIMDandVectorArchitecture/1_IntroductionToSIMDAndVectorization.html">3.1. Introduction to SIMD and Vectorization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../SIMDandVectorArchitecture/2_OpenMPSIMDConstructsAndClauses.html">3.2. OpenMP SIMD Constructs and Clauses</a></li>
<li class="toctree-l2"><a class="reference internal" href="../SIMDandVectorArchitecture/3_UtilizingSIMDDirectivesForLoopVectorization.html">3.3. Utilizing SIMD Directives for Loop Vectorization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../SIMDandVectorArchitecture/4_FunctionVectorizationWithdeclaresimd.html">3.4. Function Vectorization with <code class="docutils literal notranslate"><span class="pre">declare</span> <span class="pre">simd</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../SIMDandVectorArchitecture/5_DataAlignmentandLinearClauses.html">3.5. Data Alignment and Linear Clauses</a></li>
<li class="toctree-l2"><a class="reference internal" href="../SIMDandVectorArchitecture/6_SIMDReductionsAndScans.html">3.6. SIMD Reductions and Scans</a></li>
<li class="toctree-l2"><a class="reference internal" href="../SIMDandVectorArchitecture/7_BestPracticesAndPerformanceConsiderations.html">3.7. Best Practices and Performance Considerations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../SIMDandVectorArchitecture/8_RealWorldExamplesAndCaseStudies.html">3.8. Real-World Examples and Case Studies</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Ch4_GPUAccel.html">4. Parallel Programming for GPU Accelerators</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../GPUAccelerators/1_Introduction.html">4.1. Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../GPUAccelerators/2_OpenMPDeviceConstructs.html">4.2. OpenMP Device Constructs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../GPUAccelerators/3_MappingDataToGPUDevices.html">4.3. Mapping Data to GPU Devices</a></li>
<li class="toctree-l2"><a class="reference internal" href="../GPUAccelerators/4_AsynchronousExecutionAndDependencies.html">4.4. Asynchronous Execution and Dependencies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../GPUAccelerators/5_DeviceMemoryManagement.html">4.5. Device Memory Management</a></li>
<li class="toctree-l2"><a class="reference internal" href="../GPUAccelerators/6_ParallelExecutionOnGPUDevices.html">4.6. Parallel Execution on GPU Devices</a></li>
<li class="toctree-l2"><a class="reference internal" href="../GPUAccelerators/7_TuningPerformanceForGPUOffloading.html">4.7. Tuning Performance for GPU Offloading</a></li>
<li class="toctree-l2"><a class="reference internal" href="../GPUAccelerators/8_AdvancedTopicsAndBestPractices.html">4.8. Advanced Topics and Best Practices</a></li>
<li class="toctree-l2"><a class="reference internal" href="../GPUAccelerators/9_Conclusion.html">4.9. Conclusion</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/passlab/InteractiveOpenMPProgramming/main?urlpath=lab/tree/src/MultiCoreMultiCPU/3_UsingOpenMP_teams.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Binder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Binder logo" src="../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/passlab/InteractiveOpenMPProgramming" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/passlab/InteractiveOpenMPProgramming/issues/new?title=Issue%20on%20page%20%2FMultiCoreMultiCPU/3_UsingOpenMP_teams.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/MultiCoreMultiCPU/3_UsingOpenMP_teams.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Creating SPMD parallelism using OpenMP teams directive</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-usage">2.3.1. Basic Usage</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#teams-directive">2.3.1.1. <code class="docutils literal notranslate"><span class="pre">teams</span></code> Directive</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-creating-teams">2.3.1.2. Example: Creating Teams</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interaction-with-other-directives">2.3.2. Interaction with Other Directives</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#teams-and-distribute-directives">2.3.2.1. Teams and Distribute Directives</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#teams-and-parallel-directives">2.3.2.2. Teams and Parallel Directives</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#teams-and-target-directives">2.3.2.3. Teams and Target Directives</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">2.3.2.4. Summary</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-environment">2.3.3. Data Environment</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-sharing-attributes">2.3.3.1. Data-Sharing Attributes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-data-sharing-attributes-in-teams-region">2.3.3.2. Example: Data-Sharing Attributes in Teams Region</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">2.3.3.3. Summary</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#clauses">2.3.4. Clauses</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#num-teams-clause">2.3.4.1. num_teams Clause</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#thread-limit-clause">2.3.4.2. thread_limit Clause</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reduction-clause">2.3.4.3. reduction Clause</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#default-clause">2.3.4.4. default Clause</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#allocate-clause">2.3.4.5. allocate Clause</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#shared-clause">2.3.4.6. shared Clause</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#private-clause">2.3.4.7. private Clause</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#firstprivate-clause">2.3.4.8. firstprivate Clause</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">2.3.4.9. Summary</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#targeting-devices">2.3.5. Targeting Devices</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#offloading-with-target-and-teams-directives">2.3.5.1. Offloading with Target and Teams Directives</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#leveraging-device-parallelism">2.3.5.2. Leveraging Device Parallelism</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">2.3.5.3. Summary</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#best-practices-for-effective-use-of-teams-directive">2.3.6. Best Practices for Effective Use of Teams Directive</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimal-team-configuration">2.3.6.1. Optimal Team Configuration</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#workload-distribution">2.3.6.2. Workload Distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#minimizing-data-movement">2.3.6.3. Minimizing Data Movement</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#efficient-reductions">2.3.6.4. Efficient Reductions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#debugging-and-profiling">2.3.6.5. Debugging and Profiling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">2.3.6.6. Conclusion</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-topics">2.3.7. Advanced Topics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#nested-parallelism">2.3.7.1. Nested Parallelism</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#simd-parallelism">2.3.7.2. SIMD Parallelism</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interoperability-with-other-programming-models">2.3.7.3. Interoperability with Other Programming Models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">2.3.7.4. Summary</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">2.3.8. Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">2.3.9. Exercises</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="creating-spmd-parallelism-using-openmp-teams-directive">
<h1><span class="section-number">2.3. </span>Creating SPMD parallelism using OpenMP <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive<a class="headerlink" href="#creating-spmd-parallelism-using-openmp-teams-directive" title="Link to this heading">#</a></h1>
<p>In OpenMP, the <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive plays a crucial role, especially in the context of offloading computations to devices such as GPUs.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive in OpenMP is used to create a league of thread teams, each of which can execute concurrently. This directive is particularly useful in scenarios where a hierarchical parallelism model is desired. For example, in GPU programming, a common pattern is to distribute work among multiple teams of threads, where each team can further parallelize the work among its member threads.</p>
<p>Unlike the <code class="docutils literal notranslate"><span class="pre">parallel</span></code> directive, which is used to create a single team of threads, the <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive allows for the creation of multiple teams, providing an additional level of parallelism. This makes the <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive a powerful tool for exploiting the parallel capabilities of modern hardware architectures, especially in the context of heterogeneous computing.</p>
<p>In this chapter, we will explore the basic usage of the <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive, its interaction with other OpenMP directives, and how it can be used to target parallel execution on devices like GPUs. By the end of this chapter, you should have a solid understanding of the <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive and how to leverage it to write efficient parallel programs with OpenMP.</p>
<section id="basic-usage">
<h2><span class="section-number">2.3.1. </span>Basic Usage<a class="headerlink" href="#basic-usage" title="Link to this heading">#</a></h2>
<section id="teams-directive">
<h3><span class="section-number">2.3.1.1. </span><code class="docutils literal notranslate"><span class="pre">teams</span></code> Directive<a class="headerlink" href="#teams-directive" title="Link to this heading">#</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive indicates that the loop that follows is split among multiple thread teams, one thread team computing one part of the task. Developers can use the <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive to use a large number of thread teams.</p>
<p>The following figure shows the execution model of the <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive:</p>
<p><img alt="teams_directive" src="../_images/teams.jpeg" /></p>
<p>A league of teams is created when a thread encounters a <code class="docutils literal notranslate"><span class="pre">teams</span></code> construct. Each team is an initial team, and the initial thread in each team executes the team area.
After a team is created, the number of initial teams remains the same for the duration of the <code class="docutils literal notranslate"><span class="pre">teams</span></code> region.
Within a <code class="docutils literal notranslate"><span class="pre">teams</span></code> region, the initial team number uniquely identifies each initial team. A thread can obtain its own initial team number by calling the <em>omp_get_team_num</em> library routine.
The teams directive has the following characteristics:</p>
<ul class="simple">
<li><p>the <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive can spawn one or more thread teams with the same number of threads</p></li>
<li><p>code is portable for one thread team or multiple thread teams</p></li>
<li><p>only the primary thread of each team continues to execute</p></li>
<li><p>no synchronization between thread teams</p></li>
<li><p>programmers don’t need to think about how to decompose loops</p></li>
</ul>
<p>OpenMP was originally designed for multithreading on shared-memory parallel computers, so the parallel directive only creates a single layer of parallelism.
The team instruction is used to express the second level of scalable parallelization. Before OpenMP 5.0, it can be only used on the GPU (with an associated target construct). In OpenMP 5.0 the <code class="docutils literal notranslate"><span class="pre">teams</span></code> construct was extended to enable the host to execute a teams region.</p>
<p>The syntax for the <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive in C/C++ is:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="cp">#pragma omp teams [clause[ [,] clause] ... ] new-line</span>
<span class="w">    </span><span class="n">structured</span><span class="o">-</span><span class="n">block</span>
</pre></div>
</div>
<p>The syntax in Fortran is:</p>
<div class="highlight-fortran notranslate"><div class="highlight"><pre><span></span><span class="c">!$omp teams [clause[ [,] clause] ... ]</span>
<span class="w">    </span><span class="n">loosely</span><span class="o">-</span><span class="n">structured</span><span class="o">-</span><span class="k">block</span>
<span class="c">!$omp end teams</span>
</pre></div>
</div>
<p>In its simplest form, the <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive can be used without any clauses:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="cp">#pragma omp teams</span>
<span class="p">{</span>
<span class="w">    </span><span class="c1">// Code to be executed by each team</span>
<span class="p">}</span>
</pre></div>
</div>
<p>This will create a league of teams, with the number of teams and the number of threads per team determined by the implementation. Typically, the number of teams is chosen to match the number of available processing units, such as cores or GPU compute units.</p>
</section>
<section id="example-creating-teams">
<h3><span class="section-number">2.3.1.2. </span>Example: Creating Teams<a class="headerlink" href="#example-creating-teams" title="Link to this heading">#</a></h3>
<p>Here’s a basic example that demonstrates the use of the <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;stdio.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;omp.h&gt;</span>

<span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="cp">#pragma omp teams</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;Team %d out of %d teams</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">omp_get_team_num</span><span class="p">(),</span><span class="w"> </span><span class="n">omp_get_num_teams</span><span class="p">());</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>In this example, each team will print its team number and the total number of teams. The <code class="docutils literal notranslate"><span class="pre">omp_get_team_num()</span></code> function returns the team number of the calling thread, and the <code class="docutils literal notranslate"><span class="pre">omp_get_num_teams()</span></code> function returns the total number of teams in the current league.</p>
<p>When executed, the program might produce output similar to the following, depending on the number of teams created by the implementation:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Team</span> <span class="mi">0</span> <span class="n">out</span> <span class="n">of</span> <span class="mi">4</span> <span class="n">teams</span>
<span class="n">Team</span> <span class="mi">1</span> <span class="n">out</span> <span class="n">of</span> <span class="mi">4</span> <span class="n">teams</span>
<span class="n">Team</span> <span class="mi">2</span> <span class="n">out</span> <span class="n">of</span> <span class="mi">4</span> <span class="n">teams</span>
<span class="n">Team</span> <span class="mi">3</span> <span class="n">out</span> <span class="n">of</span> <span class="mi">4</span> <span class="n">teams</span>
</pre></div>
</div>
<p>This simple example demonstrates the basic usage of the <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive to create multiple teams of threads. In the following sections, we will explore how to control the number of teams and threads, and how the <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive interacts with other OpenMP directives for more complex parallel programming scenarios.</p>
</section>
</section>
<section id="interaction-with-other-directives">
<h2><span class="section-number">2.3.2. </span>Interaction with Other Directives<a class="headerlink" href="#interaction-with-other-directives" title="Link to this heading">#</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive in OpenMP is often used in conjunction with other directives to create a hierarchical parallelism model. This section explores how the <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive interacts with the <code class="docutils literal notranslate"><span class="pre">distribute</span></code>, <code class="docutils literal notranslate"><span class="pre">parallel</span></code>, and <code class="docutils literal notranslate"><span class="pre">target</span></code> directives.</p>
<section id="teams-and-distribute-directives">
<h3><span class="section-number">2.3.2.1. </span>Teams and Distribute Directives<a class="headerlink" href="#teams-and-distribute-directives" title="Link to this heading">#</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">distribute</span></code> directive is used to distribute the iterations of a loop across the teams created by the <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive. This combination is particularly useful for data parallelism where each team works on a different portion of the data.</p>
<p><strong>Example:</strong> Distributing loop iterations across teams to perform a parallel reduction.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;omp.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;stdio.h&gt;</span>

<span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1000</span><span class="p">;</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">array</span><span class="p">[</span><span class="n">N</span><span class="p">];</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">sum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>

<span class="w">    </span><span class="c1">// Initialize the array with some values</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">array</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">i</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="cp">#pragma omp target teams distribute reduction(+:sum)</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">sum</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">array</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;Sum: %d</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">sum</span><span class="p">);</span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>In this example, the loop iterations are distributed across teams, and each team contributes to the reduction operation to calculate the sum of the array elements.</p>
</section>
<section id="teams-and-parallel-directives">
<h3><span class="section-number">2.3.2.2. </span>Teams and Parallel Directives<a class="headerlink" href="#teams-and-parallel-directives" title="Link to this heading">#</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive can also be combined with the <code class="docutils literal notranslate"><span class="pre">parallel</span></code> directive to create nested parallelism. Within each team, the <code class="docutils literal notranslate"><span class="pre">parallel</span></code> directive creates a parallel region where multiple threads can work concurrently.</p>
<p><strong>Example:</strong> Using nested parallelism with the <code class="docutils literal notranslate"><span class="pre">teams</span></code> and <code class="docutils literal notranslate"><span class="pre">parallel</span></code> directives to perform matrix multiplication.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;omp.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;stdio.h&gt;</span>

<span class="cp">#define N 1000</span>
<span class="cp">#define M 1000</span>
<span class="cp">#define P 1000</span>

<span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kt">double</span><span class="w"> </span><span class="n">A</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">M</span><span class="p">],</span><span class="w"> </span><span class="n">B</span><span class="p">[</span><span class="n">M</span><span class="p">][</span><span class="n">P</span><span class="p">],</span><span class="w"> </span><span class="n">C</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">P</span><span class="p">];</span>

<span class="w">    </span><span class="c1">// Initialize matrices A and B</span>
<span class="w">    </span><span class="c1">// ...</span>

<span class="w">    </span><span class="cp">#pragma omp target teams</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="cp">#pragma omp distribute parallel for collapse(2)</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">P</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="kt">double</span><span class="w"> </span><span class="n">sum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0</span><span class="p">;</span>
<span class="w">                </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">M</span><span class="p">;</span><span class="w"> </span><span class="n">k</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                    </span><span class="n">sum</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">k</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="n">j</span><span class="p">];</span>
<span class="w">                </span><span class="p">}</span>
<span class="w">                </span><span class="n">C</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sum</span><span class="p">;</span>
<span class="w">            </span><span class="p">}</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="c1">// Print matrix C</span>
<span class="w">    </span><span class="c1">// ...</span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>In this example, the outer loop iterations (over <code class="docutils literal notranslate"><span class="pre">i</span></code> and <code class="docutils literal notranslate"><span class="pre">j</span></code>) are distributed across teams, and within each team, the loop iterations are executed in parallel by the team’s threads.</p>
</section>
<section id="teams-and-target-directives">
<h3><span class="section-number">2.3.2.3. </span>Teams and Target Directives<a class="headerlink" href="#teams-and-target-directives" title="Link to this heading">#</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive is commonly used with the <code class="docutils literal notranslate"><span class="pre">target</span></code> directive for offloading computations to a device, such as a GPU. The <code class="docutils literal notranslate"><span class="pre">target</span></code> directive specifies that the code block should be executed on the device, and the <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive creates teams of threads on the device to execute the code.</p>
<p><strong>Example:</strong> Offloading a computation to a device and using teams to perform the computation in parallel.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;omp.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;stdio.h&gt;</span>

<span class="cp">#define N 1000</span>

<span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">A</span><span class="p">[</span><span class="n">N</span><span class="p">],</span><span class="w"> </span><span class="n">B</span><span class="p">[</span><span class="n">N</span><span class="p">],</span><span class="w"> </span><span class="n">C</span><span class="p">[</span><span class="n">N</span><span class="p">];</span>

<span class="w">    </span><span class="c1">// Initialize arrays A and B</span>
<span class="w">    </span><span class="c1">// ...</span>

<span class="w">    </span><span class="cp">#pragma omp target teams map(to: A, B) map(from: C)</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="cp">#pragma omp distribute parallel for</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">C</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">B</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="c1">// Print array C</span>
<span class="w">    </span><span class="c1">// ...</span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>In this example, the computation of the element-wise sum of two arrays is offloaded to a device. The <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive is used to create teams of threads on the device, and the <code class="docutils literal notranslate"><span class="pre">distribute</span> <span class="pre">parallel</span> <span class="pre">for</span></code> construct is used to distribute the loop iterations across the teams and execute them in parallel.</p>
</section>
<section id="summary">
<h3><span class="section-number">2.3.2.4. </span>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive provides a flexible way to create a hierarchical parallelism model in OpenMP. By combining the <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive with the <code class="docutils literal notranslate"><span class="pre">distribute</span></code>, <code class="docutils literal notranslate"><span class="pre">parallel</span></code>, and <code class="docutils literal notranslate"><span class="pre">target</span></code> directives, developers can efficiently utilize the parallel capabilities of both CPUs and GPUs for a wide range of applications.</p>
</section>
</section>
<section id="data-environment">
<h2><span class="section-number">2.3.3. </span>Data Environment<a class="headerlink" href="#data-environment" title="Link to this heading">#</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive in OpenMP creates a new data environment for the region of code it encloses. This section discusses the data-sharing attributes and other aspects of the data environment created by the <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive.</p>
<section id="data-sharing-attributes">
<h3><span class="section-number">2.3.3.1. </span>Data-Sharing Attributes<a class="headerlink" href="#data-sharing-attributes" title="Link to this heading">#</a></h3>
<p>Within a <code class="docutils literal notranslate"><span class="pre">teams</span></code> region, variables can have one of the following data-sharing attributes: shared, private, firstprivate, or reduction. The default data-sharing attribute for variables in a <code class="docutils literal notranslate"><span class="pre">teams</span></code> region is determined by the default clause if present; otherwise, it follows the rules of the OpenMP specification.</p>
<ul class="simple">
<li><p><strong>Shared:</strong> Variables with the shared attribute are shared among all threads in all teams. Each team accesses the same storage location for shared variables.</p></li>
<li><p><strong>Private:</strong> Variables with the private attribute have a separate copy for each thread. Each thread in each team has its own copy of private variables.</p></li>
<li><p><strong>Firstprivate:</strong> Similar to private, but each thread’s copy is initialized with the value of the variable before entering the <code class="docutils literal notranslate"><span class="pre">teams</span></code> region.</p></li>
<li><p><strong>Reduction:</strong> Variables with the reduction attribute are subject to a reduction operation at the end of the <code class="docutils literal notranslate"><span class="pre">teams</span></code> region.</p></li>
</ul>
</section>
<section id="example-data-sharing-attributes-in-teams-region">
<h3><span class="section-number">2.3.3.2. </span>Example: Data-Sharing Attributes in Teams Region<a class="headerlink" href="#example-data-sharing-attributes-in-teams-region" title="Link to this heading">#</a></h3>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;omp.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;stdio.h&gt;</span>

<span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">shared_var</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">100</span><span class="p">;</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">private_var</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">200</span><span class="p">;</span>

<span class="w">    </span><span class="cp">#pragma omp target teams map(shared_var) private(private_var)</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// Inside the teams region</span>
<span class="w">        </span><span class="n">private_var</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">omp_get_team_num</span><span class="p">();</span>
<span class="w">        </span><span class="cp">#pragma omp parallel reduction(+:shared_var)</span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="n">shared_var</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">private_var</span><span class="p">;</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;Shared variable: %d</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">shared_var</span><span class="p">);</span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>In this example, <code class="docutils literal notranslate"><span class="pre">shared_var</span></code> is shared among all teams and threads, while <code class="docutils literal notranslate"><span class="pre">private_var</span></code> is private to each thread. The value of <code class="docutils literal notranslate"><span class="pre">shared_var</span></code> is modified through a reduction operation within the parallel region inside the teams region.</p>
</section>
<section id="id1">
<h3><span class="section-number">2.3.3.3. </span>Summary<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<p>Understanding the data environment and data-sharing attributes in a <code class="docutils literal notranslate"><span class="pre">teams</span></code> region is crucial for writing correct and efficient parallel programs with OpenMP. Properly managing the data environment ensures that data is correctly shared or privatized among teams and threads, avoiding data races and other concurrency issues.</p>
</section>
</section>
<section id="clauses">
<h2><span class="section-number">2.3.4. </span>Clauses<a class="headerlink" href="#clauses" title="Link to this heading">#</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive in OpenMP can be used with several clauses to control the behavior of the teams and the data environment. This section discusses the clauses with the <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive.</p>
<section id="num-teams-clause">
<h3><span class="section-number">2.3.4.1. </span>num_teams Clause<a class="headerlink" href="#num-teams-clause" title="Link to this heading">#</a></h3>
<p><strong>Syntax:</strong> <code class="docutils literal notranslate"><span class="pre">num_teams(integer-expression)</span></code></p>
<p>The <code class="docutils literal notranslate"><span class="pre">num_teams</span></code> clause specifies the number of teams to be created in the teams region.</p>
<p><strong>Example:</strong></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;omp.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;stdio.h&gt;</span>

<span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="cp">#pragma omp target teams num_teams(4)</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;Team %d out of %d teams</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">omp_get_team_num</span><span class="p">(),</span><span class="w"> </span><span class="n">omp_get_num_teams</span><span class="p">());</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="thread-limit-clause">
<h3><span class="section-number">2.3.4.2. </span>thread_limit Clause<a class="headerlink" href="#thread-limit-clause" title="Link to this heading">#</a></h3>
<p><strong>Syntax:</strong> <code class="docutils literal notranslate"><span class="pre">thread_limit(integer-expression)</span></code></p>
<p>The <code class="docutils literal notranslate"><span class="pre">thread_limit</span></code> clause specifies the maximum number of threads that can be part of each team.</p>
<p><strong>Example:</strong></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;omp.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;stdio.h&gt;</span>

<span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="cp">#pragma omp target teams thread_limit(8)</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="cp">#pragma omp parallel</span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;Thread %d in team %d</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">omp_get_thread_num</span><span class="p">(),</span><span class="w"> </span><span class="n">omp_get_team_num</span><span class="p">());</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="reduction-clause">
<h3><span class="section-number">2.3.4.3. </span>reduction Clause<a class="headerlink" href="#reduction-clause" title="Link to this heading">#</a></h3>
<p><strong>Syntax:</strong> <code class="docutils literal notranslate"><span class="pre">reduction(operator:</span> <span class="pre">list)</span></code></p>
<p>The <code class="docutils literal notranslate"><span class="pre">reduction</span></code> clause is used to perform a reduction operation on variables that are private to each thread but shared across the teams.</p>
<p><strong>Example:</strong></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;omp.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;stdio.h&gt;</span>

<span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">sum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>

<span class="w">    </span><span class="cp">#pragma omp target teams map(tofrom: sum) reduction(+:sum)</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="cp">#pragma omp parallel</span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="n">sum</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">omp_get_thread_num</span><span class="p">();</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;Sum: %d</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">sum</span><span class="p">);</span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="default-clause">
<h3><span class="section-number">2.3.4.4. </span>default Clause<a class="headerlink" href="#default-clause" title="Link to this heading">#</a></h3>
<p><strong>Syntax:</strong> <code class="docutils literal notranslate"><span class="pre">default(shared</span> <span class="pre">|</span> <span class="pre">none)</span></code></p>
<p>The <code class="docutils literal notranslate"><span class="pre">default</span></code> clause sets the default data-sharing attribute for variables in the teams region.</p>
<p><strong>Example:</strong></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;omp.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;stdio.h&gt;</span>

<span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="cp">#pragma omp target teams default(none)</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">x</span><span class="p">;</span>
<span class="w">        </span><span class="c1">// Code that uses the variable x</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="allocate-clause">
<h3><span class="section-number">2.3.4.5. </span>allocate Clause<a class="headerlink" href="#allocate-clause" title="Link to this heading">#</a></h3>
<p><strong>Syntax:</strong> <code class="docutils literal notranslate"><span class="pre">allocate([allocator:]</span> <span class="pre">list)</span></code></p>
<p>The <code class="docutils literal notranslate"><span class="pre">allocate</span></code> clause specifies the allocator to be used for the variables in the list.</p>
<p><strong>Example:</strong></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;omp.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;stdio.h&gt;</span>

<span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="cp">#pragma omp target teams allocate(x)</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// Code that uses the variable x</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="shared-clause">
<h3><span class="section-number">2.3.4.6. </span>shared Clause<a class="headerlink" href="#shared-clause" title="Link to this heading">#</a></h3>
<p><strong>Syntax:</strong> <code class="docutils literal notranslate"><span class="pre">shared(list)</span></code></p>
<p>The <code class="docutils literal notranslate"><span class="pre">shared</span></code> clause specifies that the data within a parallel region is shared among all threads.</p>
<p><strong>Example:</strong></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;omp.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;stdio.h&gt;</span>

<span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="cp">#pragma omp target teams shared(x)</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// All threads in the team share the variable x</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="private-clause">
<h3><span class="section-number">2.3.4.7. </span>private Clause<a class="headerlink" href="#private-clause" title="Link to this heading">#</a></h3>
<p><strong>Syntax:</strong> <code class="docutils literal notranslate"><span class="pre">private(list)</span></code></p>
<p>The <code class="docutils literal notranslate"><span class="pre">private</span></code> clause specifies that each thread should have its own instance of a variable.</p>
<p><strong>Example:</strong></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;omp.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;stdio.h&gt;</span>

<span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="cp">#pragma omp target teams private(x)</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// Each thread has its own private copy of x</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="firstprivate-clause">
<h3><span class="section-number">2.3.4.8. </span>firstprivate Clause<a class="headerlink" href="#firstprivate-clause" title="Link to this heading">#</a></h3>
<p><strong>Syntax:</strong> <code class="docutils literal notranslate"><span class="pre">firstprivate(list)</span></code></p>
<p>The <code class="docutils literal notranslate"><span class="pre">firstprivate</span></code> clause provides each thread with its own copy of a variable and initializes each copy with the value of the variable at the time the parallel region is entered.</p>
<p><strong>Example:</strong></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;omp.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;stdio.h&gt;</span>

<span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="cp">#pragma omp target teams firstprivate(x)</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// Each thread has its own copy of x, initialized with the value of x</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="id2">
<h3><span class="section-number">2.3.4.9. </span>Summary<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<p>Clauses provide a powerful way to customize the behavior of the <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive in OpenMP. By using clauses like <code class="docutils literal notranslate"><span class="pre">num_teams</span></code>, <code class="docutils literal notranslate"><span class="pre">thread_limit</span></code>, <code class="docutils literal notranslate"><span class="pre">reduction</span></code>, <code class="docutils literal notranslate"><span class="pre">default</span></code>, <code class="docutils literal notranslate"><span class="pre">allocate</span></code>, <code class="docutils literal notranslate"><span class="pre">shared</span></code>, <code class="docutils literal notranslate"><span class="pre">private</span></code>, and <code class="docutils literal notranslate"><span class="pre">firstprivate</span></code>, programmers can control various aspects of the teams’ behavior and the data environment.</p>
</section>
</section>
<section id="targeting-devices">
<h2><span class="section-number">2.3.5. </span>Targeting Devices<a class="headerlink" href="#targeting-devices" title="Link to this heading">#</a></h2>
<p>In modern computing, harnessing the power of specialized hardware such as Graphics Processing Units (GPUs) is essential for achieving high performance in parallel computing. OpenMP provides a seamless way to offload computations to these devices using the <code class="docutils literal notranslate"><span class="pre">target</span></code> and <code class="docutils literal notranslate"><span class="pre">teams</span></code> directives.</p>
<section id="offloading-with-target-and-teams-directives">
<h3><span class="section-number">2.3.5.1. </span>Offloading with Target and Teams Directives<a class="headerlink" href="#offloading-with-target-and-teams-directives" title="Link to this heading">#</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">target</span></code> directive instructs the compiler to execute the enclosed code block on a specified device, typically a GPU. The <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive, when used in conjunction with <code class="docutils literal notranslate"><span class="pre">target</span></code>, creates a league of thread teams to execute the code in parallel on the device.</p>
<p><strong>Example: Vector Addition on a GPU</strong></p>
<p>Consider the following example where two arrays are added element-wise and the result is stored in a third array:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;omp.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;stdio.h&gt;</span>

<span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1000</span><span class="p">;</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">a</span><span class="p">[</span><span class="n">N</span><span class="p">],</span><span class="w"> </span><span class="n">b</span><span class="p">[</span><span class="n">N</span><span class="p">],</span><span class="w"> </span><span class="n">c</span><span class="p">[</span><span class="n">N</span><span class="p">];</span>

<span class="w">    </span><span class="c1">// Initialize the input arrays</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mf">1.0f</span><span class="p">;</span>
<span class="w">        </span><span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mf">2.0f</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="c1">// Offload the computation to a GPU</span>
<span class="w">    </span><span class="cp">#pragma omp target teams map(to: a, b) map(from: c)</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="cp">#pragma omp distribute parallel for</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="c1">// Output the result</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;%f &quot;</span><span class="p">,</span><span class="w"> </span><span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>In this example, the <code class="docutils literal notranslate"><span class="pre">target</span> <span class="pre">teams</span></code> directive offloads the addition of arrays <code class="docutils literal notranslate"><span class="pre">a</span></code> and <code class="docutils literal notranslate"><span class="pre">b</span></code> to a GPU. The <code class="docutils literal notranslate"><span class="pre">map</span></code> clauses specify how data is transferred between the host and the device. The <code class="docutils literal notranslate"><span class="pre">distribute</span> <span class="pre">parallel</span> <span class="pre">for</span></code> directive ensures that the computation is performed in parallel by the teams of threads on the device.</p>
</section>
<section id="leveraging-device-parallelism">
<h3><span class="section-number">2.3.5.2. </span>Leveraging Device Parallelism<a class="headerlink" href="#leveraging-device-parallelism" title="Link to this heading">#</a></h3>
<p>Using the <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive with the <code class="docutils literal notranslate"><span class="pre">target</span></code> directive enables you to harness the parallel processing capabilities of devices like GPUs. It allows for scalable parallel execution where the workload is distributed across multiple teams of threads, each executing concurrently on the device.</p>
</section>
<section id="id3">
<h3><span class="section-number">2.3.5.3. </span>Summary<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<p>The integration of the <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive with the <code class="docutils literal notranslate"><span class="pre">target</span></code> directive in OpenMP provides a powerful and flexible approach to offloading computations to devices. This capability allows programmers to efficiently utilize the parallel processing power of GPUs and other devices, thereby enhancing the performance of parallel applications. In the following chapters, we will delve deeper into the <code class="docutils literal notranslate"><span class="pre">map</span></code> clause and explore other aspects of device offloading in OpenMP.</p>
</section>
</section>
<section id="best-practices-for-effective-use-of-teams-directive">
<h2><span class="section-number">2.3.6. </span>Best Practices for Effective Use of Teams Directive<a class="headerlink" href="#best-practices-for-effective-use-of-teams-directive" title="Link to this heading">#</a></h2>
<p>Optimizing the use of the <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive in OpenMP is crucial for harnessing the full potential of parallel computing, especially when offloading computations to devices such as GPUs. Here are some essential best practices to keep in mind:</p>
<section id="optimal-team-configuration">
<h3><span class="section-number">2.3.6.1. </span>Optimal Team Configuration<a class="headerlink" href="#optimal-team-configuration" title="Link to this heading">#</a></h3>
<p>Selecting the appropriate number of teams is pivotal. It should be aligned with the target device’s architecture and the computational workload. For GPUs, a rule of thumb is to create a number of teams that corresponds to a multiple of the device’s compute units.</p>
</section>
<section id="workload-distribution">
<h3><span class="section-number">2.3.6.2. </span>Workload Distribution<a class="headerlink" href="#workload-distribution" title="Link to this heading">#</a></h3>
<p>Achieving a balanced distribution of work among teams is key to avoiding performance bottlenecks. Utilize the <code class="docutils literal notranslate"><span class="pre">distribute</span></code> directive in tandem with <code class="docutils literal notranslate"><span class="pre">teams</span></code> to ensure an equitable assignment of loop iterations or other computational tasks across the teams.</p>
<p><strong>Example: Workload Distribution</strong></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;omp.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;stdio.h&gt;</span>

<span class="cp">#define N 10000</span>

<span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">array</span><span class="p">[</span><span class="n">N</span><span class="p">];</span>

<span class="w">    </span><span class="c1">// Initialize the array</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">array</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">i</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="c1">// Distribute workload evenly among teams</span>
<span class="w">    </span><span class="cp">#pragma omp target teams map(tofrom: array)</span>
<span class="w">    </span><span class="cp">#pragma omp distribute</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">array</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">array</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mf">2.0f</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="minimizing-data-movement">
<h3><span class="section-number">2.3.6.3. </span>Minimizing Data Movement<a class="headerlink" href="#minimizing-data-movement" title="Link to this heading">#</a></h3>
<p>Data transfer between the host and the device can significantly impact performance. Employ the <code class="docutils literal notranslate"><span class="pre">map</span></code> clause with precision to reduce unnecessary data movement. Additionally, explore techniques like data prefetching and overlapping data transfers with computation to enhance data locality.</p>
<p><strong>Example: Efficient Data Movement</strong></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;omp.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;stdio.h&gt;</span>

<span class="cp">#define N 10000</span>

<span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">input</span><span class="p">[</span><span class="n">N</span><span class="p">],</span><span class="w"> </span><span class="n">output</span><span class="p">[</span><span class="n">N</span><span class="p">];</span>

<span class="w">    </span><span class="c1">// Initialize the input array</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">input</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">i</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="c1">// Minimize data movement in offloading</span>
<span class="w">    </span><span class="cp">#pragma omp target teams map(to: input) map(from: output)</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="cp">#pragma omp distribute parallel for</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">output</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">input</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mf">2.0f</span><span class="p">;</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="efficient-reductions">
<h3><span class="section-number">2.3.6.4. </span>Efficient Reductions<a class="headerlink" href="#efficient-reductions" title="Link to this heading">#</a></h3>
<p>Ensure that reductions are performed efficiently, especially when using the <code class="docutils literal notranslate"><span class="pre">reduction</span></code> clause with the <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive. Implement parallel reduction algorithms that leverage the device’s parallel processing capabilities to avoid bottlenecks.</p>
<p><strong>Example: Parallel Reduction</strong></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;omp.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;stdio.h&gt;</span>

<span class="cp">#define N 10000</span>

<span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">array</span><span class="p">[</span><span class="n">N</span><span class="p">];</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">sum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">;</span>

<span class="w">    </span><span class="c1">// Initialize the array</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">array</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">i</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="c1">// Efficient reduction on the device</span>
<span class="w">    </span><span class="cp">#pragma omp target teams map(to: array) reduction(+:sum)</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="cp">#pragma omp distribute parallel for</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">sum</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">array</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;Sum: %f</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">sum</span><span class="p">);</span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="debugging-and-profiling">
<h3><span class="section-number">2.3.6.5. </span>Debugging and Profiling<a class="headerlink" href="#debugging-and-profiling" title="Link to this heading">#</a></h3>
<p>Leverage OpenMP-aware debugging and profiling tools to identify and address performance bottlenecks. Regularly verify the correctness of parallel execution to ensure the expected behavior of your application.</p>
</section>
<section id="conclusion">
<h3><span class="section-number">2.3.6.6. </span>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h3>
<p>Adhering to these best practices will enable you to maximize the efficiency and correctness of your parallel applications using the <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive in OpenMP. Tailor your approach to the specific requirements of your application and the capabilities of your target device for optimal results.</p>
</section>
</section>
<section id="advanced-topics">
<h2><span class="section-number">2.3.7. </span>Advanced Topics<a class="headerlink" href="#advanced-topics" title="Link to this heading">#</a></h2>
<p>While the basic usage of the <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive provides a powerful tool for parallel computing, there are several advanced topics that can further enhance the performance and flexibility of your OpenMP applications. Here are some advanced topics related to the <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive:</p>
<section id="nested-parallelism">
<h3><span class="section-number">2.3.7.1. </span>Nested Parallelism<a class="headerlink" href="#nested-parallelism" title="Link to this heading">#</a></h3>
<p>OpenMP supports nested parallelism, where a parallel region can be defined inside another parallel region. This can be particularly useful when using the <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive to create a hierarchical parallelism model.</p>
<p><strong>Example: Nested Parallelism</strong></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;omp.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;stdio.h&gt;</span>

<span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="cp">#pragma omp target teams</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// Outer parallel region executed by teams</span>
<span class="w">        </span><span class="cp">#pragma omp parallel</span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="c1">// Inner parallel region executed by threads within each team</span>
<span class="w">            </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;Team %d, Thread %d</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">omp_get_team_num</span><span class="p">(),</span><span class="w"> </span><span class="n">omp_get_thread_num</span><span class="p">());</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>In this example, an outer parallel region is created by the <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive, and an inner parallel region is created by the <code class="docutils literal notranslate"><span class="pre">parallel</span></code> directive. Each team of threads executes the inner parallel region independently.</p>
</section>
<section id="simd-parallelism">
<h3><span class="section-number">2.3.7.2. </span>SIMD Parallelism<a class="headerlink" href="#simd-parallelism" title="Link to this heading">#</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive can be combined with SIMD (Single Instruction, Multiple Data) constructs to exploit vector parallelism on devices that support SIMD instructions.</p>
<p><strong>Example: SIMD Parallelism</strong></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;omp.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;stdio.h&gt;</span>

<span class="cp">#define N 10000</span>

<span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">array</span><span class="p">[</span><span class="n">N</span><span class="p">];</span>

<span class="w">    </span><span class="c1">// Initialize the array</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">array</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">i</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="c1">// Use SIMD parallelism within teams</span>
<span class="w">    </span><span class="cp">#pragma omp target teams map(tofrom: array)</span>
<span class="w">    </span><span class="cp">#pragma omp distribute</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="cp">#pragma omp simd</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">array</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">array</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mf">1.0f</span><span class="p">;</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>In this example, the <code class="docutils literal notranslate"><span class="pre">simd</span></code> directive is used within the loop to enable SIMD parallelism, allowing multiple elements of the array to be processed simultaneously by SIMD instructions.</p>
</section>
<section id="interoperability-with-other-programming-models">
<h3><span class="section-number">2.3.7.3. </span>Interoperability with Other Programming Models<a class="headerlink" href="#interoperability-with-other-programming-models" title="Link to this heading">#</a></h3>
<p>OpenMP can interoperate with other programming models, such as MPI (Message Passing Interface) or CUDA, to take advantage of specific features of different parallel programming approaches.</p>
<p><strong>Example: Interoperability with MPI</strong></p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;omp.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;mpi.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;stdio.h&gt;</span>

<span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">argc</span><span class="p">,</span><span class="w"> </span><span class="kt">char</span><span class="w"> </span><span class="o">*</span><span class="n">argv</span><span class="p">[])</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">MPI_Init</span><span class="p">(</span><span class="o">&amp;</span><span class="n">argc</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">argv</span><span class="p">);</span>

<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">rank</span><span class="p">;</span>
<span class="w">    </span><span class="n">MPI_Comm_rank</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">rank</span><span class="p">);</span>

<span class="w">    </span><span class="cp">#pragma omp target teams</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;MPI Rank %d, Team %d</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">rank</span><span class="p">,</span><span class="w"> </span><span class="n">omp_get_team_num</span><span class="p">());</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="n">MPI_Finalize</span><span class="p">();</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>In this example, OpenMP is used in conjunction with MPI, where each MPI process executes an OpenMP <code class="docutils literal notranslate"><span class="pre">teams</span></code> region on a device. This allows for a hybrid parallel programming approach that combines distributed memory parallelism (MPI) with shared memory parallelism (OpenMP).</p>
</section>
<section id="id4">
<h3><span class="section-number">2.3.7.4. </span>Summary<a class="headerlink" href="#id4" title="Link to this heading">#</a></h3>
<p>Exploring advanced topics related to the <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive can unlock new levels of performance and flexibility in your OpenMP applications. Whether it’s leveraging nested parallelism, exploiting SIMD instructions, or interoperating with other programming models, these advanced techniques can help you tackle complex parallel computing challenges.</p>
</section>
</section>
<section id="id5">
<h2><span class="section-number">2.3.8. </span>Summary<a class="headerlink" href="#id5" title="Link to this heading">#</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive is a powerful feature in OpenMP that enables the creation of a league of thread teams for parallel execution, particularly on devices such as GPUs. This chapter has covered the fundamental concepts, usage, and best practices associated with the <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive, along with advanced topics for further exploration.</p>
<p>Key takeaways from this chapter include:</p>
<ul class="simple">
<li><p><strong>Basic Usage:</strong> The <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive, often used in conjunction with the <code class="docutils literal notranslate"><span class="pre">target</span></code> directive, allows for offloading computations to a device with a specified number of teams and threads.</p></li>
<li><p><strong>Data Environment:</strong> Understanding and managing the data environment, including data-sharing attributes and clauses like <code class="docutils literal notranslate"><span class="pre">allocate</span></code>, <code class="docutils literal notranslate"><span class="pre">default</span></code>, <code class="docutils literal notranslate"><span class="pre">shared</span></code>, and <code class="docutils literal notranslate"><span class="pre">private</span></code>, is crucial for efficient parallel execution.</p></li>
<li><p><strong>Targeting Devices:</strong> The combination of <code class="docutils literal notranslate"><span class="pre">target</span></code> and <code class="docutils literal notranslate"><span class="pre">teams</span></code> directives provides a flexible and powerful approach to offload computations to devices, with the <code class="docutils literal notranslate"><span class="pre">map</span></code> clause playing a vital role in managing data movement.</p></li>
<li><p><strong>Best Practices:</strong> To maximize performance and ensure correct execution, it’s important to choose the right number of teams, balance the workload among teams, minimize data movement, use reductions efficiently, and employ debugging and profiling tools.</p></li>
<li><p><strong>Advanced Topics:</strong> Exploring nested parallelism, SIMD parallelism, and interoperability with other programming models can further enhance the capabilities of your OpenMP applications.</p></li>
</ul>
<p>By mastering the use of the <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive, you can unlock new levels of parallelism and performance in your applications, making them more scalable and efficient on modern computing architectures.</p>
</section>
<section id="exercises">
<h2><span class="section-number">2.3.9. </span>Exercises<a class="headerlink" href="#exercises" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p><strong>Nested Parallelism:</strong> Modify the example code for nested parallelism to create a hierarchy of parallel regions using the <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive. Experiment with different numbers of teams and threads to observe the performance impact.</p></li>
<li><p><strong>SIMD Parallelism:</strong> Implement a matrix multiplication algorithm using SIMD parallelism within the <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive. Compare the performance of the SIMD-parallelized version with a non-parallelized version.</p></li>
<li><p><strong>Load Balancing:</strong> Write a program that performs a computation on a large array and uses the <code class="docutils literal notranslate"><span class="pre">distribute</span></code> directive to distribute the workload among teams. Experiment with different load balancing strategies to minimize load imbalance.</p></li>
<li><p><strong>Reduction Operation:</strong> Modify the example code for the reduction clause to perform a more complex reduction operation, such as finding the maximum or minimum value in an array, using the <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive.</p></li>
<li><p><strong>Interoperability:</strong> Explore interoperability between OpenMP and another parallel programming model, such as MPI or CUDA. Write a program that combines both programming models to perform a parallel computation.</p></li>
<li><p><strong>Debugging and Profiling:</strong> Use an OpenMP-aware debugging or profiling tool to analyze the performance of one of the previous exercises. Identify any performance bottlenecks and propose optimizations.</p></li>
<li><p><strong>Optimization Strategies:</strong> Experiment with different optimization strategies, such as loop unrolling, loop tiling, or data prefetching, within the <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive to improve the performance of a parallelized computation.</p></li>
<li><p><strong>Real-world Application:</strong> Identify a real-world application that can benefit from parallelization using the <code class="docutils literal notranslate"><span class="pre">teams</span></code> directive. Write a program that parallelizes a relevant part of the application and measure the performance improvement.</p></li>
<li><p><strong>Scalability Analysis:</strong> Conduct a scalability analysis of one of the previous exercises by varying the problem size and the number of teams/threads. Determine the scalability limits of your parallelized code.</p></li>
<li><p><strong>Advanced Topics Exploration:</strong> Choose one of the advanced topics discussed in this chapter, such as nested parallelism or SIMD parallelism, and explore it further by implementing a more complex example or conducting a detailed performance analysis.</p></li>
</ol>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./MultiCoreMultiCPU"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="2_UsingOpenMP_parallel.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">2.2. </span>Creating SPMD parallelism using OpenMP <strong>parallel</strong> directive</p>
      </div>
    </a>
    <a class="right-next"
       href="4_SynchronizationThreads.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">2.4. </span>Synchronization of Threads Using Barrier and Ordered Directive</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-usage">2.3.1. Basic Usage</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#teams-directive">2.3.1.1. <code class="docutils literal notranslate"><span class="pre">teams</span></code> Directive</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-creating-teams">2.3.1.2. Example: Creating Teams</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interaction-with-other-directives">2.3.2. Interaction with Other Directives</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#teams-and-distribute-directives">2.3.2.1. Teams and Distribute Directives</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#teams-and-parallel-directives">2.3.2.2. Teams and Parallel Directives</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#teams-and-target-directives">2.3.2.3. Teams and Target Directives</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">2.3.2.4. Summary</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-environment">2.3.3. Data Environment</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-sharing-attributes">2.3.3.1. Data-Sharing Attributes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-data-sharing-attributes-in-teams-region">2.3.3.2. Example: Data-Sharing Attributes in Teams Region</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">2.3.3.3. Summary</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#clauses">2.3.4. Clauses</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#num-teams-clause">2.3.4.1. num_teams Clause</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#thread-limit-clause">2.3.4.2. thread_limit Clause</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reduction-clause">2.3.4.3. reduction Clause</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#default-clause">2.3.4.4. default Clause</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#allocate-clause">2.3.4.5. allocate Clause</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#shared-clause">2.3.4.6. shared Clause</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#private-clause">2.3.4.7. private Clause</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#firstprivate-clause">2.3.4.8. firstprivate Clause</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">2.3.4.9. Summary</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#targeting-devices">2.3.5. Targeting Devices</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#offloading-with-target-and-teams-directives">2.3.5.1. Offloading with Target and Teams Directives</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#leveraging-device-parallelism">2.3.5.2. Leveraging Device Parallelism</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">2.3.5.3. Summary</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#best-practices-for-effective-use-of-teams-directive">2.3.6. Best Practices for Effective Use of Teams Directive</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimal-team-configuration">2.3.6.1. Optimal Team Configuration</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#workload-distribution">2.3.6.2. Workload Distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#minimizing-data-movement">2.3.6.3. Minimizing Data Movement</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#efficient-reductions">2.3.6.4. Efficient Reductions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#debugging-and-profiling">2.3.6.5. Debugging and Profiling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">2.3.6.6. Conclusion</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-topics">2.3.7. Advanced Topics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#nested-parallelism">2.3.7.1. Nested Parallelism</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#simd-parallelism">2.3.7.2. SIMD Parallelism</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interoperability-with-other-programming-models">2.3.7.3. Interoperability with Other Programming Models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">2.3.7.4. Summary</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">2.3.8. Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">2.3.9. Exercises</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Xinyao Yi, Anjia Wang, Yonghong Yan and Chunhua Liao
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>